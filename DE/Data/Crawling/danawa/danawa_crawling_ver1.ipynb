{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n"
     ]
    }
   ],
   "source": [
    "#삭제 ㄴㄴ\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_url(query, date_range, page):\n",
    "    base_url = \"https://search.danawa.com/dsearch.php\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'registerDateRange': date_range,\n",
    "        'page': page\n",
    "    }\n",
    "    url = base_url + '?' + '&'.join([f'{k}={v}' for k, v in params.items()])\n",
    "    return url\n",
    "\n",
    "def scrape_data(url):\n",
    "    # WebDriver 인스턴스 생성 (Chrome 브라우저)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # 페이지 로드\n",
    "    driver.get(url)\n",
    "\n",
    "    # 페이지가 완전히 로드될 때까지 대기\n",
    "    time.sleep(5)\n",
    "\n",
    "    # 페이지 소스를 가져옴\n",
    "    html = driver.page_source\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # WebDriver 종료\n",
    "    driver.quit()\n",
    "\n",
    "    # 스크래핑한 데이터를 저장할 리스트 초기화\n",
    "    data_list = []\n",
    "\n",
    "    # 상품 리스트 컨테이너 요소 찾기\n",
    "    product_container = soup.find('div', class_='main_prodlist main_prodlist_list')\n",
    "\n",
    "    # 상품 아이템들 찾기\n",
    "    if product_container:\n",
    "        li_elements = product_container.find_all('li', class_='prod_item')\n",
    "\n",
    "        # 각 상품 아이템에서 필요한 정보 추출\n",
    "        for li_element in li_elements:\n",
    "            data = {}\n",
    "            try:\n",
    "                # 상품 번호 추출\n",
    "                product_id = li_element['id'].replace('productItem', '')\n",
    "                data['상품번호'] = product_id\n",
    "\n",
    "                # 추가 정보 추출\n",
    "                category_info_elem = li_element.find('input', id=f'productItem_categoryInfo_{product_id}')\n",
    "                data['카테고리정보'] = category_info_elem.get('value') if category_info_elem else ''\n",
    "\n",
    "                min_price_elem = li_element.find('input', id=f'min_price_{product_id}')\n",
    "                data['최소가격'] = min_price_elem.get('value') if min_price_elem else ''\n",
    "\n",
    "                prod_name_elem = li_element.find('p', class_='prod_name')\n",
    "                data['상품명'] = prod_name_elem.text.strip() if prod_name_elem else ''\n",
    "\n",
    "                spec_list_elem = li_element.find('div', class_='spec_list')\n",
    "                data['상세스펙'] = spec_list_elem.text.strip() if spec_list_elem else ''\n",
    "\n",
    "                # 등록월 추출\n",
    "                reg_month_elem = li_element.find('dl', class_='meta_item mt_date')\n",
    "                reg_month_dd_elem = reg_month_elem.find('dd') if reg_month_elem else None\n",
    "                data['등록월'] = reg_month_dd_elem.text.strip() if reg_month_dd_elem else ''\n",
    "\n",
    "                # 상품의견 추출\n",
    "                opinion_elem = li_element.find('dl', class_='meta_item mt_comment')\n",
    "                opinion_div_elem = opinion_elem.find('div', class_='cnt_opinion') if opinion_elem else None\n",
    "                data['상품의견'] = opinion_div_elem.text.strip() if opinion_div_elem else ''\n",
    "\n",
    "                # 가격 추출 - 여러 개인 경우 모두 추출\n",
    "                price_sect_elems = li_element.find_all('p', class_='price_sect')\n",
    "                prices = [price.text.strip() for price in price_sect_elems]\n",
    "                data['가격'] = prices\n",
    "\n",
    "                # 메모리 추출 - 여러 개인 경우 모두 추출\n",
    "                memory_sect_elems = li_element.find_all('p', class_='memory_sect')\n",
    "                memories = [memory.text.strip().replace('\\t',', ') for memory in memory_sect_elems]\n",
    "                data['메모리'] = memories\n",
    "\n",
    "                thumb_image_elem = li_element.find('div', class_='thumb_image')\n",
    "                thumb_img_elem = thumb_image_elem.find('img') if thumb_image_elem else None\n",
    "                data['이미지URL'] = thumb_img_elem.get('src') if thumb_img_elem else ''\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "            # 딕셔너리를 데이터 리스트에 추가\n",
    "            data_list.append(data)\n",
    "\n",
    "    # 딕셔너리의 리스트를 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n"
     ]
    }
   ],
   "source": [
    "query = '삼성갤럭시북'\n",
    "date_range = '202201~202203'\n",
    "pages = 1  # 스크래핑할 페이지 수\n",
    "\n",
    "# 모든 페이지의 데이터를 스크래핑하여 하나의 데이터프레임으로 병합\n",
    "dfs = []\n",
    "for page in range(1, pages+1):\n",
    "    url = get_url(query, date_range, page)\n",
    "    df = scrape_data(url)\n",
    "    dfs.append(df)\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "result_df.to_csv('삼성갤럭시북_data.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n"
     ]
    }
   ],
   "source": [
    "query = 'lg그램'\n",
    "date_range = '202201~202203'\n",
    "pages = 1  # 스크래핑할 페이지 수\n",
    "\n",
    "# 모든 페이지의 데이터를 스크래핑하여 하나의 데이터프레임으로 병합\n",
    "dfs = []\n",
    "for page in range(1, pages+1):\n",
    "    url = get_url(query, date_range, page)\n",
    "    df = scrape_data(url)\n",
    "    dfs.append(df)\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "result_df.to_csv('lg_그램_data.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '맥북'\n",
    "date_range = '202201~202203'\n",
    "pages = 1  # 스크래핑할 페이지 수\n",
    "\n",
    "# 모든 페이지의 데이터를 스크래핑하여 하나의 데이터프레임으로 병합\n",
    "dfs = []\n",
    "for page in range(1, pages+1):\n",
    "    url = get_url(query, date_range, page)\n",
    "    df = scrape_data(url)\n",
    "    dfs.append(df)\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "result_df.to_csv('맥북_data.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n",
      "An error occurred: 'id'\n"
     ]
    }
   ],
   "source": [
    "query = '애플워치'\n",
    "date_range = '202201~202203'\n",
    "pages = 1  # 스크래핑할 페이지 수\n",
    "\n",
    "# 모든 페이지의 데이터를 스크래핑하여 하나의 데이터프레임으로 병합\n",
    "dfs = []\n",
    "for page in range(1, pages+1):\n",
    "    url = get_url(query, date_range, page)\n",
    "    df = scrape_data(url)\n",
    "    dfs.append(df)\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장\n",
    "result_df.to_csv('애플워치_data.csv', index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataengineer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
